{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c103567c8226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# set chdir to current dir\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.realpath(os.path.dirname(__file__)))\n",
    "os.chdir(os.path.realpath(os.path.dirname(__file__)))\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import sqlite3\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from unidecode import unidecode\n",
    "import time\n",
    "from threading import Lock, Timer\n",
    "import pandas as pd\n",
    "from config import stop_words\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "import string\n",
    "import pickle\n",
    "import itertools\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#consumer key, consumer secret, access token, access secret.\n",
    "# https://apps.twitter.com/ to setup\n",
    "ckey=\"\"\n",
    "csecret=\"\"\n",
    "atoken=\"\"\n",
    "asecret=\"\n",
    "# isolation lever disables automatic transactions,\n",
    "# we are disabling thread check as we are creating connection here, but we'll be inserting from a separate thread (no need for serialization)\n",
    "conn = sqlite3.connect('twitter.db', isolation_level=None, check_same_thread=False)\n",
    "c = conn.cursor()\n",
    "\n",
    "def create_table():\n",
    "    try:\n",
    "\n",
    "        # http://www.sqlite.org/pragma.html#pragma_journal_mode\n",
    "        # for us - it allows concurrent write and reads\n",
    "        c.execute(\"PRAGMA journal_mode=wal\")\n",
    "        c.execute(\"PRAGMA wal_checkpoint=TRUNCATE\")\n",
    "        #c.execute(\"PRAGMA journal_mode=PERSIST\")\n",
    "\n",
    "        # changed unix to INTEGER (it is integer, sqlite can use up to 8-byte long integers)\n",
    "        c.execute(\"CREATE TABLE IF NOT EXISTS sentiment(id INTEGER PRIMARY KEY AUTOINCREMENT, unix INTEGER, tweet TEXT, sentiment REAL)\")\n",
    "        # key-value table for random stuff\n",
    "        c.execute(\"CREATE TABLE IF NOT EXISTS misc(key TEXT PRIMARY KEY, value TEXT)\")\n",
    "        # id on index, both as DESC (as you are sorting in DESC order)\n",
    "        c.execute(\"CREATE INDEX id_unix ON sentiment (id DESC, unix DESC)\")\n",
    "        # out full-text search table, i choosed creating data from external (content) table - sentiment\n",
    "        # instead of directly inserting to that table, as we are saving more data than just text\n",
    "        # https://sqlite.org/fts5.html - 4.4.2\n",
    "        c.execute(\"CREATE VIRTUAL TABLE sentiment_fts USING fts5(tweet, content=sentiment, content_rowid=id, prefix=1, prefix=2, prefix=3)\")\n",
    "        # that trigger will automagically update out table when row is interted\n",
    "        # (requires additional triggers on update and delete)\n",
    "        c.execute(\"\"\"\n",
    "            CREATE TRIGGER sentiment_insert AFTER INSERT ON sentiment BEGIN\n",
    "                INSERT INTO sentiment_fts(rowid, tweet) VALUES (new.id, new.tweet);\n",
    "            END\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "create_table()\n",
    "\n",
    "# create lock\n",
    "lock = Lock()\n",
    "\n",
    "class listener(StreamListener):\n",
    "\n",
    "    data = []\n",
    "    lock = None\n",
    "\n",
    "    def __init__(self, lock):\n",
    "\n",
    "        # create lock\n",
    "        self.lock = lock\n",
    "\n",
    "        # init timer for database save\n",
    "        self.save_in_database()\n",
    "\n",
    "        # call __inint__ of super class\n",
    "        super().__init__()\n",
    "\n",
    "    def save_in_database(self):\n",
    "\n",
    "        # set a timer (1 second)\n",
    "        Timer(1, self.save_in_database).start()\n",
    "\n",
    "        # with lock, if there's data, save in transaction using one bulk query\n",
    "        with self.lock:\n",
    "            if len(self.data):\n",
    "                c.execute('BEGIN TRANSACTION')\n",
    "                try:\n",
    "                    c.executemany(\"INSERT INTO sentiment (unix, tweet, sentiment) VALUES (?, ?, ?)\", self.data)\n",
    "                except:\n",
    "                    pass\n",
    "                c.execute('COMMIT')\n",
    "\n",
    "                self.data = []\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            #print('data')\n",
    "            data = json.loads(data)\n",
    "            # there are records like that:\n",
    "            # {'limit': {'track': 14667, 'timestamp_ms': '1520216832822'}}\n",
    "            if 'truncated' not in data:\n",
    "                #print(data)\n",
    "                return True\n",
    "            if data['truncated']:\n",
    "                tweet = unidecode(data['extended_tweet']['full_text'])\n",
    "            else:\n",
    "                tweet = unidecode(data['text'])\n",
    "            time_ms = data['timestamp_ms']\n",
    "            vs = analyzer.polarity_scores(tweet)\n",
    "            sentiment = vs['compound']\n",
    "            #print(time_ms, tweet, sentiment)\n",
    "\n",
    "            # append to data list (to be saved every 1 second)\n",
    "            with self.lock:\n",
    "                self.data.append((time_ms, tweet, sentiment))\n",
    "\n",
    "        except KeyError as e:\n",
    "            #print(data)\n",
    "            print(str(e))\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "\n",
    "# make a counter with blacklist words and empty word with some big value - we'll use it later to filter counter\n",
    "stop_words.append('')\n",
    "blacklist_counter = Counter(dict(zip(stop_words, [1000000] * len(stop_words))))\n",
    "\n",
    "# complie a regex for split operations (punctuation list, plus space and new line)\n",
    "punctuation = [str(i) for i in string.punctuation]\n",
    "split_regex = re.compile(\"[ \\n\" + re.escape(\"\".join(punctuation)) + ']')\n",
    "\n",
    "def map_nouns(col):\n",
    "    return [word[0] for word in TextBlob(col).tags if word[1] == u'NNP']\n",
    "\n",
    "# generate \"trending\"\n",
    "def generate_trending():\n",
    "\n",
    "    try:\n",
    "        # select last 10k tweets\n",
    "        df = pd.read_sql(\"SELECT * FROM sentiment ORDER BY id DESC, unix DESC LIMIT 10000\", conn)\n",
    "        df['nouns'] = list(map(map_nouns,df['tweet']))\n",
    "\n",
    "        # make tokens\n",
    "        tokens = split_regex.split(' '.join(list(itertools.chain.from_iterable(df['nouns'].values.tolist()))).lower())\n",
    "        # clean and get top 10\n",
    "        trending = (Counter(tokens) - blacklist_counter).most_common(10)\n",
    "\n",
    "        # get sentiments\n",
    "        trending_with_sentiment = {}\n",
    "        for term, count in trending:\n",
    "            df = pd.read_sql(\"SELECT sentiment.* FROM  sentiment_fts fts LEFT JOIN sentiment ON fts.rowid = sentiment.id WHERE fts.sentiment_fts MATCH ? ORDER BY fts.rowid DESC LIMIT 1000\", conn, params=(term,))\n",
    "            trending_with_sentiment[term] = [df['sentiment'].mean(), count]\n",
    "\n",
    "        # save in a database\n",
    "        with lock:\n",
    "            c.execute('BEGIN TRANSACTION')\n",
    "            try:\n",
    "                c.execute(\"REPLACE INTO misc (key, value) VALUES ('trending', ?)\", (pickle.dumps(trending_with_sentiment),))\n",
    "            except:\n",
    "                pass\n",
    "            c.execute('COMMIT')\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        with open('errors.txt','a') as f:\n",
    "            f.write(str(e))\n",
    "            f.write('\\n')\n",
    "    finally:\n",
    "        Timer(5, generate_trending).start()\n",
    "\n",
    "Timer(1, generate_trending).start()\n",
    "\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "        auth = OAuthHandler(ckey, csecret)\n",
    "        auth.set_access_token(atoken, asecret)\n",
    "        twitterStream = Stream(auth, listener(lock))\n",
    "        twitterStream.filter(track=[\"a\",\"e\",\"i\",\"o\",\"u\"])\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bitcfde0aadf4d74095b2141961b274842c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
